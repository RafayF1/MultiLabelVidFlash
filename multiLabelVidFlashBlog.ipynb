{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9439ac4a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Multi-Label Video Classification using *PyTorch Lightning Flash*\"\n",
    "author:\n",
    "  - name: \"Rafay Farhan\"\n",
    "    affiliations:\n",
    "      - name: \"DreamAI\"\n",
    "\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088400b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73933916",
   "metadata": {},
   "source": [
    "## Small Intro To Multi-Label Video Classification and the main problem\n",
    "\n",
    "Multi-Label classification is the task of assigning multiple labels to each data instance, in this case, videos. A given input may belong to more than one label depending on the data, as opposed to the single-label assignment in traditional classification. Now, video classification stands as one of the most crucial challenges in computer vision. Video content comprises temporally related images, and this temporal dimension introduces a new layer of complexity to the image classification problem. There are various ways to achieve this, for instance, by utilizing the [**Pytorch Lightning Flash**](https://lightning-flash.readthedocs.io/en/latest/quickstart.html) API. You can refer to our previous blog where we created a Video Classification pipeline using one of the larger [X3D](https://pytorch.org/hub/facebookresearch_pytorchvideo_x3d/) models. However, a current drawback of Lightning Flash is the absence of tutorials on how to train a Multi-Label classification video model anywhere on the internet. In general, not many APIs have worked on this either. So, how do we achieve this? It involves a deep dive into the behind-the-scenes code of Flash, customization, and then bringing it all together!\n",
    "\n",
    "The repository for this blog can be found [**here**](https://github.com/RafayF1/MultiLabelVidFlash.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f2a68",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b002c6",
   "metadata": {},
   "source": [
    "## The Root of the problem\n",
    "\n",
    "Before we embark on our journey to explore the code and address the problem, we need to understand precisely what is hindering us from performing **Multi-Label Video classification** in Lightning Flash. As mentioned earlier, the Flash documentation lacks a tutorial for this specific task. Interestingly, there is a tutorial for [Multi-Label Image Classification](https://lightning-flash.readthedocs.io/en/latest/reference/image_classification_multi_label.html), which raises the question: why can't we similarly create a _Multi-label video classification model_? Let's compare some code to find out.\n",
    "\n",
    "For image classification, Flash employs its [**ImageClassifier**](https://lightning-flash.readthedocs.io/en/latest/api/generated/flash.image.classification.model.ImageClassifier.html#flash.image.classification.model.ImageClassifier) class, which supports multi-label classification through the use of the _multi-label_ argument. However, when examining the arguments in the [source code of the **VideoClassifier**](https://lightning-flash.readthedocs.io/en/latest/api/generated/flash.video.classification.model.VideoClassifier.html#flash.video.classification.model.VideoClassifier) used for video classification, you'll notice the absence of a _multi-label_ argument. This is the primary issue, suggesting that Flash does not inherently support Multi-Label Video Classification. So, can we simply resolve the problem by adding an argument? Well, not quite. After exploring the behind-the-scenes code, we'll discover that we need to write some custom code. Nevertheless, we have identified the root of the problem, enabling us to take a step-by-step approach to achieve our goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943bd5b9",
   "metadata": {},
   "source": [
    "## The Steps to do\n",
    "\n",
    "0. All the desired **imports** and **helper functions**\n",
    "\n",
    "1. **Data Pre-processing**: This might differ for people depending on the dataset they work on the common goal is to make a multi-label dataset that the VideoClassificationData will use to create the DataModule.\n",
    "\n",
    "2. **Creating our Custom Transform**: This will allow us to use the **x3d_m** model while also integrating the multi-label dataset into the DataModule.\n",
    "\n",
    "3. **Creating a Custom Classifier class**: The real magic will happen here. We will also add a loss function suitable for multi-label approach.\n",
    "\n",
    "4. **Define our DataModule**\n",
    "\n",
    "5. **Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a996d9a",
   "metadata": {},
   "source": [
    "## 0. Imports and Helper Functions\n",
    "\n",
    "All of the libraries, classes and functions we need for data pre-processing. **imports.py** has all the necessary imports needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e64d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafay_farhan/.local/lib/python3.7/site-packages/torchvision/models/_utils.py:253: UserWarning: Accessing the model URLs via the internal dictionary of the module is deprecated since 0.13 and may be removed in the future. Please access them via the appropriate Weights Enum instead.\n",
      "  \"Accessing the model URLs via the internal dictionary of the module is deprecated since 0.13 and may \"\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e390ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash.core.classification import ClassificationAdapterTask\n",
    "from types import FunctionType\n",
    "from typing import Any, Dict, Iterable, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DistributedSampler\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import flash\n",
    "from flash.core.classification import ClassificationTask\n",
    "from flash.core.data.io.input import DataKeys\n",
    "from flash.core.registry import FlashRegistry\n",
    "from flash.core.utilities.compatibility import accelerator_connector\n",
    "from flash.core.utilities.imports import _PYTORCHVIDEO_AVAILABLE\n",
    "from flash.core.utilities.providers import _PYTORCHVIDEO\n",
    "from flash.core.utilities.types import (\n",
    "    LOSS_FN_TYPE,\n",
    "    LR_SCHEDULER_TYPE,\n",
    "    METRICS_TYPE,\n",
    "    OPTIMIZER_TYPE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324564a8",
   "metadata": {},
   "source": [
    "**Helper Functions**\n",
    "\n",
    "These functions will help us in pre-processing data, all of these are in **utils.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155b128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2889d85f",
   "metadata": {},
   "source": [
    "## 1. Data Pre-Processing\n",
    "\n",
    "One of the most integral aspects of machine learning is data pre-processing. It plays a key role in allowing us to comprehend the data, understand it, its functionality, and manipulate or process it to suit our needs. Now, since this tutorial primarily aims to demonstrate how we can achieve **Multi-Label Video Classification** in Flash, I won't be using a large dataset, nor a perfectly suitable one for that matter. Instead, we will use the same [**Kinetics**](https://paperswithcode.com/dataset/kinetics-400-1) dataset featured in the [Video Classification Tutorial](https://lightning-flash.readthedocs.io/en/latest/reference/video_classification.html) on the documentation website, but with a slight twist. We'll convert this data into Multi-Label by adding some labels of our own. Let's break it down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab2449",
   "metadata": {},
   "source": [
    "Firstly, we will download the data using **download_data** magic function provided by Lightning Flash.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7ff8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(\"https://pl-flash-data.s3.amazonaws.com/kinetics.zip\", \"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b09c42",
   "metadata": {},
   "source": [
    "Once the data is downloaded, a folder by the name \"**data**\" will be created in your home directory. Here’s an outline of the folder structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dataset\n",
    "├── train\n",
    "│   ├── archery\n",
    "│   │   ├── -1q7jA3DXQM_000005_000015.mp4\n",
    "│   │   ├── -5NN5hdIwTc_000036_000046.mp4\n",
    "│   │   ...\n",
    "│   ├── bowling\n",
    "│   │   ├── -5ExwuF5IUI_000030_000040.mp4\n",
    "│   │   ├── -7sTNNI1Bcg_000075_000085.mp4\n",
    "│   ... ...\n",
    "└── val\n",
    "    ├── archery\n",
    "    │   ├── 0S-P4lr_c7s_000022_000032.mp4\n",
    "    │   ├── 2x1lIrgKxYo_000589_000599.mp4\n",
    "    │   ...\n",
    "    ├── bowling\n",
    "    │   ├── 1W7HNDBA4pA_000002_000012.mp4\n",
    "    │   ├── 4JxH3S5JwMs_000003_000013.mp4\n",
    "    ... ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e206973c",
   "metadata": {},
   "source": [
    "It appears quite straightforward, easily readable, and accessible. However, as we need to manipulate the data, we must obtain the path names of the video files stored in each folder. To retrieve all of these paths, we will employ one of our helper functions, **get_files**, which will store all of the paths of the video files in a single list variable. Let's access the train files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52bb1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = Path(\"./data/kinetics/train/\")\n",
    "train_vids = get_files(train_data_path, extensions=[\".mp4\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64488f25",
   "metadata": {},
   "source": [
    "**train_vids** is a list that contains the paths of every .mp4 (i.e video) files in the train folder. How about a look at one of these paths?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6424c90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/kinetics/train/bowling/-N4vEATi9Mk_000003_000013.mp4')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vids[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e48450",
   "metadata": {},
   "source": [
    "Using **Path** from the **pathlib** library in Python, it allows us to break down file path names and manipulate them. For instance, this is how you get the name of the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c49b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-N4vEATi9Mk_000003_000013.mp4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vids[1].name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078af9d2",
   "metadata": {},
   "source": [
    "Just like showed in the folder structure above, this is the name of a video file. We can also get just the name of the folder (or label in our case) this file belongs to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54eef90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bowling'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vids[1].parent.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0656766",
   "metadata": {},
   "source": [
    "Let's take a look at the labels in the data. Currently, the dataset has 5 labels: **archery**, **bowling**, **flying_kite**, **high_jump**, **marching**. As we know this is a single label classification data. To make it multi-label, we are going to add two new labels: _indoor_ and _outdoor_. The former states which of these actions are usually done indoor while the latter states which of these actions are performed outdoor. We will consider **archery** and **bowling** as _indoor_ actions while **flying_kite**, **high_jump**, **marching** will be categorized as _outdoor_ actions. This is the theoritical part of it, now how de we go on to achieve this?\n",
    "\n",
    "We usually _one hot_ encode our labels for multi-class classification problems. In one hot encoding, we represent the categorical variables as binary vectors. We first map categorical values to integer values. Then, each integer value is represented as a binary vector where all values are zero except the index of the integer, which is marked with a 1. However, we know that for multi-label classification problems, we can have any number of classes associated with it. We'll assume that the labels are mutually exclusive, and thus, instead of one hot encoding, we'll try **multi-label binarization**. Here the label (which can have multiple classes) is transformed into a binary vector such that all values are zero except the indexes associated with each class in that label, which is marked with a 1.\n",
    "\n",
    "Let's have a look at our 7 labels. Imagine them as a list like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2079cb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archery',\n",
       " 'bowling',\n",
       " 'flying_kite',\n",
       " 'high_jump',\n",
       " 'marching',\n",
       " 'indoor',\n",
       " 'outdoor']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: False\n",
    "\n",
    "labels = [\n",
    "    \"archery\",\n",
    "    \"bowling\",\n",
    "    \"flying_kite\",\n",
    "    \"high_jump\",\n",
    "    \"marching\",\n",
    "    \"indoor\",\n",
    "    \"outdoor\",\n",
    "]\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae03d94",
   "metadata": {},
   "source": [
    "Looking at the list above, we can say that \"archery\" is at the list[0] position. \"bowling\" at list[1] and so on. Using this logic, we will try **multi-label binarization**. Now, we have got all of the paths of the train video files. We can also get the label name of the video file through **parent.name** Combining all of it together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026398b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for v in train_vids:\n",
    "    n = v.parent.name\n",
    "    if n == \"archery\":\n",
    "        lab = [1, 0, 0, 0, 0, 1, 0]\n",
    "    elif n == \"bowling\":\n",
    "        lab = [0, 1, 0, 0, 0, 1, 0]\n",
    "    elif n == \"flying_kite\":\n",
    "        lab = [0, 0, 1, 0, 0, 0, 1]\n",
    "    elif n == \"high_jump\":\n",
    "        lab = [0, 0, 0, 1, 0, 0, 1]\n",
    "    elif n == \"marching\":\n",
    "        lab = [0, 0, 0, 0, 1, 0, 1]\n",
    "    l.append(lab)\n",
    "\n",
    "l = np.array(l)\n",
    "l = torch.from_numpy(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56a2e9",
   "metadata": {},
   "source": [
    "Here is how our list of tensors to use for multi-label training looks like (viewing the first 5 elements):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e071d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fb536",
   "metadata": {},
   "source": [
    "_Voila!_ This is exactly what we needed. _1_ states that the video belongs to these labels while _0_ states the opposite. Since this is multi-label classification, a video can belong to more than one class, which is precisely what we wanted. With that accomplished, let's create a DataFrame to view our processed data in all of its glory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a13de54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vids = [str(vid).replace(str(train_data_path) + \"/\", \"\") for vid in train_vids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca921b76",
   "metadata": {},
   "source": [
    "To create the DataFrame, we will slice our 2-D tensor numpy array accordingly with the sequence of label names, for example, as stated before, \"archery\" is the 0th element which means l[:,0] would extract column 0 (the first column) from all of the rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53196a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"video\": train_vids,\n",
    "        \"archery\": l[:, 0],\n",
    "        \"bowling\": l[:, 1],\n",
    "        \"flying_kite\": l[:, 2],\n",
    "        \"high_jump\": l[:, 3],\n",
    "        \"marching\": l[:, 4],\n",
    "        \"indoor\": l[:, 5],\n",
    "        \"outdoor\": l[:, 6],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb39c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e9e61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>archery</th>\n",
       "      <th>bowling</th>\n",
       "      <th>flying_kite</th>\n",
       "      <th>high_jump</th>\n",
       "      <th>marching</th>\n",
       "      <th>indoor</th>\n",
       "      <th>outdoor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>high_jump/-ZEThexrAe0_000002_000012.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>high_jump/-v6Dj_-drts_000003_000013.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>marching/-534IANO-AM_000120_000130.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>marching/-4c4r9YeS6s_000098_000108.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>flying_kite/-cMsP8DzCls_000019_000029.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        video  archery  bowling  flying_kite  \\\n",
       "18    high_jump/-ZEThexrAe0_000002_000012.mp4        0        0            0   \n",
       "16    high_jump/-v6Dj_-drts_000003_000013.mp4        0        0            0   \n",
       "38     marching/-534IANO-AM_000120_000130.mp4        0        0            0   \n",
       "33     marching/-4c4r9YeS6s_000098_000108.mp4        0        0            0   \n",
       "25  flying_kite/-cMsP8DzCls_000019_000029.mp4        0        0            1   \n",
       "\n",
       "    high_jump  marching  indoor  outdoor  \n",
       "18          1         0       0        1  \n",
       "16          1         0       0        1  \n",
       "38          0         1       0        1  \n",
       "33          0         1       0        1  \n",
       "25          0         0       0        1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff183a",
   "metadata": {},
   "source": [
    "Removing the **.head()** will allow you to view the whole DataFrame but even this shows how our processed data looks like now. We will save the dataframe into a csv file in the train folder of our dataset which will be used in our DataModule:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cbed9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"./data/kinetics/train/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e82b4d",
   "metadata": {},
   "source": [
    "These were all integral steps to take to view our data, how it is structured, manipulating the path names of the data; which finally allowed us to **Multi-Class Binarize** the data to convert it to multi-label. Having said that, clean, organized and abstract code is a vital part of writing code. It also allows us to save time and this is a good case study for that. If you look at the dataset, we also have a **val** folder so we also need to create a csv file for that to be used in our DataModule. Instead of going through the whole process again, what if we create a function which will only take the desired **data path** and create the csv file in accordance with that? In the **utility.py** file, there is a fucntion named **createMultiLabelDf**, this is how it looks like:\n",
    "\n",
    "These were all integral steps to take to view our data, understand its structure, and manipulate the file path names, ultimately enabling us to **Multi-Class Binarize** the data and convert it to a multi-label format. That being said, clean, organized, and abstract code is a vital aspect of coding. It not only saves time but also enables us to write efficient code. When examining the dataset, you'll notice the existence of a **val** folder, which also requires the creation of a CSV file for use in our DataModule. Rather than going through the entire process again, what if we create a function that takes the desired _data path_ as input and generates the CSV file accordingly? In the **utility.py** file, there is a function named **createMultiLabelDf**, and this is what it looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22168f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMultiLabelDf(data_path):\n",
    "    data_path = Path(data_path)\n",
    "    vids = get_files(data_path, extensions=[\".mp4\"])\n",
    "    l = multiBinary(vids)\n",
    "    vids = [str(vid).replace(str(data_path) + \"/\", \"\") for vid in vids]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"video\": vids,\n",
    "            \"archery\": l[:, 0],\n",
    "            \"bowling\": l[:, 1],\n",
    "            \"flying_kite\": l[:, 2],\n",
    "            \"high_jump\": l[:, 3],\n",
    "            \"marching\": l[:, 4],\n",
    "            \"indoor\": l[:, 5],\n",
    "            \"outdoor\": l[:, 6],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df.to_csv(str(data_path) + \"/\" + str(data_path.name) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e490858",
   "metadata": {},
   "source": [
    "This function contains everything we did above in chronological order. Now see the magic happen, we will firstly, create two variables that will have the string path of the train and val folders saved. Next, we will just pass these as arguments to the function twice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2fb4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./data/kinetics/train/\"\n",
    "val_data_path = \"./data/kinetics/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d19c7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = createMultiLabelDf(train_data_path)\n",
    "val_csv = createMultiLabelDf(val_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ad308",
   "metadata": {},
   "source": [
    "## 2. Creating our Custom Transform\n",
    "\n",
    "The reason why we are creating our own **Custom Transform** is because we want to use the **x3d_m** model which gives far more accurate and better results than the smaller **x3d_xs** model. I would suggest giving this [tutorial](https://medium.com/@dreamai/video-classification-using-pytorch-lightning-flash-and-the-x3d-family-of-models-ec6361969073) a read for an in-depth concept. Another huge reason, in the multi-label aspect of things is this: if we explore the code behind-the-scenes, this is what the currently staticly written transform looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc416117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassificationInputTransform(InputTransform):\n",
    "    image_size: int = 244\n",
    "    temporal_sub_sample: int = 8\n",
    "    mean: Tensor = torch.tensor([0.45, 0.45, 0.45])\n",
    "    std: Tensor = torch.tensor([0.225, 0.225, 0.225])\n",
    "    data_format: str = \"BCTHW\"\n",
    "    same_on_frame: bool = False\n",
    "\n",
    "    def per_sample_transform(self) -> Callable:\n",
    "        per_sample_transform = [CenterCrop(self.image_size)]\n",
    "\n",
    "        return ApplyToKeys(\n",
    "            DataKeys.INPUT,\n",
    "            Compose(\n",
    "                [UniformTemporalSubsample(self.temporal_sub_sample), normalize]\n",
    "                + per_sample_transform\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def train_per_sample_transform(self) -> Callable:\n",
    "        per_sample_transform = [RandomCrop(self.image_size, pad_if_needed=True)]\n",
    "\n",
    "        return ApplyToKeys(\n",
    "            DataKeys.INPUT,\n",
    "            Compose(\n",
    "                [UniformTemporalSubsample(self.temporal_sub_sample), normalize]\n",
    "                + per_sample_transform\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def per_batch_transform_on_device(self) -> Callable:\n",
    "        return ApplyToKeys(\n",
    "            DataKeys.INPUT,\n",
    "            K.VideoSequential(\n",
    "                K.Normalize(self.mean, self.std),\n",
    "                data_format=self.data_format,\n",
    "                same_on_frame=self.same_on_frame,\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b7559",
   "metadata": {},
   "source": [
    "We already know to use the **x3d_m** model we only need to change the **temporal_sub_sample** value. However, we already know that our new processed data contains a tensor numpy array for every video which indicates which class or _TARGET_ it belongs to. This is why another vital adjustment we need to do is to allow our DataModule to convert the input targets to tensors. Otherwise, it will generate errors. For this. we will use the **[ApplyToKeys](https://lightning-flash.readthedocs.io/en/latest/api/generated/flash.core.data.io.input_transform.InputTransform.html)** transform that Lightning Flash provides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x: Tensor) -> Tensor:\n",
    "    return x / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7c1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataModule(InputTransform):\n",
    "    image_size: int = 256\n",
    "    temporal_sub_sample: int = 16\n",
    "    mean: Tensor = torch.tensor([0.45, 0.45, 0.45])\n",
    "    std: Tensor = torch.tensor([0.225, 0.225, 0.225])\n",
    "    data_format: str = \"BCTHW\"\n",
    "    same_on_frame: bool = False\n",
    "\n",
    "    def per_sample_transform(self) -> Callable:\n",
    "        per_sample_transform = [CenterCrop(self.image_size)]\n",
    "\n",
    "        return Compose(\n",
    "            [\n",
    "                ApplyToKeys(\n",
    "                    DataKeys.INPUT,\n",
    "                    Compose(\n",
    "                        [UniformTemporalSubsample(self.temporal_sub_sample), normalize]\n",
    "                        + per_sample_transform\n",
    "                    ),\n",
    "                ),\n",
    "                ApplyToKeys(DataKeys.TARGET, torch.as_tensor),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def train_per_sample_transform(self) -> Callable:\n",
    "        per_sample_transform = [RandomCrop(self.image_size, pad_if_needed=True)]\n",
    "\n",
    "        return Compose(\n",
    "            [\n",
    "                ApplyToKeys(\n",
    "                    DataKeys.INPUT,\n",
    "                    Compose(\n",
    "                        [UniformTemporalSubsample(self.temporal_sub_sample), normalize]\n",
    "                        + per_sample_transform\n",
    "                    ),\n",
    "                ),\n",
    "                ApplyToKeys(DataKeys.TARGET, torch.as_tensor),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def per_batch_transform_on_device(self) -> Callable:\n",
    "        return ApplyToKeys(\n",
    "            DataKeys.INPUT,\n",
    "            K.VideoSequential(\n",
    "                K.Normalize(self.mean, self.std),\n",
    "                data_format=self.data_format,\n",
    "                same_on_frame=self.same_on_frame,\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60983e",
   "metadata": {},
   "source": [
    "## 3. Creating a Custom Classifier Class\n",
    "\n",
    "As Pytorch Lightning Flash directly does not allow us to perform _Multi-Label Video Classification_ we will need to write some custom code inherting its concepts and the classes of Flash. The first step is to determine which loss function to use. Since, we are not dealing with Single-Label classification, we must use a different one. Torch provides a function [**binary_cross_entropy_with_logits**](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html) that measures Binary Cross Entropy between target and input logits. This fits perfeclty in our case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_with_logits(x: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"Calls BCE with logits and cast the target one_hot (y) encoding to floating point precision.\"\"\"\n",
    "    return F.binary_cross_entropy_with_logits(x, y.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2dc409",
   "metadata": {},
   "source": [
    "This is some necessary code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a65b7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VIDEO_CLASSIFIER_BACKBONES = FlashRegistry(\"backbones\")\n",
    "\n",
    "if _PYTORCHVIDEO_AVAILABLE:\n",
    "    from pytorchvideo.models import hub\n",
    "\n",
    "    for fn_name in dir(hub):\n",
    "        if \"__\" not in fn_name:\n",
    "            fn = getattr(hub, fn_name)\n",
    "            if isinstance(fn, FunctionType):\n",
    "                _VIDEO_CLASSIFIER_BACKBONES(fn=fn, providers=_PYTORCHVIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d8313",
   "metadata": {},
   "source": [
    "Now, we write our custom classifer class that will be used to load our desired model for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce7488a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VC(ClassificationTask):\n",
    "    backbones: FlashRegistry = _VIDEO_CLASSIFIER_BACKBONES\n",
    "\n",
    "    required_extras = \"video\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: Optional[int] = None,\n",
    "        multi_label: bool = False,\n",
    "        labels: Optional[List[str]] = None,\n",
    "        backbone: Union[str, nn.Module] = \"x3d_xs\",\n",
    "        backbone_kwargs: Optional[Dict] = None,\n",
    "        pretrained: bool = True,\n",
    "        loss_fn: LOSS_FN_TYPE = binary_cross_entropy_with_logits,\n",
    "        optimizer: OPTIMIZER_TYPE = \"Adam\",\n",
    "        lr_scheduler: LR_SCHEDULER_TYPE = None,\n",
    "        metrics: METRICS_TYPE = Accuracy(),\n",
    "        learning_rate: Optional[float] = None,\n",
    "        head: Optional[Union[FunctionType, nn.Module]] = None,\n",
    "    ):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if labels is not None and num_classes is None:\n",
    "            num_classes = len(labels)\n",
    "\n",
    "        super().__init__(\n",
    "            model=None,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            metrics=metrics,\n",
    "            learning_rate=learning_rate,\n",
    "            num_classes=num_classes,\n",
    "            labels=labels,\n",
    "            multi_label=multi_label,\n",
    "        )\n",
    "\n",
    "        if not backbone_kwargs:\n",
    "            backbone_kwargs = {}\n",
    "\n",
    "        backbone_kwargs[\"pretrained\"] = (\n",
    "            True if (flash._IS_TESTING and torch.cuda.is_available()) else pretrained\n",
    "        )\n",
    "        backbone_kwargs[\"head_activation\"] = None\n",
    "\n",
    "        if isinstance(backbone, nn.Module):\n",
    "            self.backbone = backbone\n",
    "        elif isinstance(backbone, str):\n",
    "            self.backbone = self.backbones.get(backbone)(**backbone_kwargs)\n",
    "            num_features = self.backbone.blocks[-1].proj.out_features\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"backbone should be either a string or a nn.Module. Found: {backbone}\"\n",
    "            )\n",
    "\n",
    "        self.head = head or nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features, num_classes),\n",
    "        )\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        if accelerator_connector(self.trainer).is_distributed:\n",
    "            encoded_dataset = self.trainer.train_dataloader.loaders.dataset.data\n",
    "            encoded_dataset._video_sampler = DistributedSampler(\n",
    "                encoded_dataset._labeled_videos\n",
    "            )\n",
    "        super().on_train_start()\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        if accelerator_connector(self.trainer).is_distributed:\n",
    "            encoded_dataset = self.trainer.train_dataloader.loaders.dataset.data\n",
    "            encoded_dataset._video_sampler.set_epoch(self.trainer.current_epoch)\n",
    "        super().on_train_epoch_start()\n",
    "\n",
    "    def step(self, batch: Any, batch_idx: int, metrics) -> Any:\n",
    "        return super().step(\n",
    "            (batch[DataKeys.INPUT], batch[DataKeys.TARGET]), batch_idx, metrics\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Any) -> Any:\n",
    "        x = self.backbone(x)\n",
    "        if self.head is not None:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\n",
    "        predictions = self(batch[DataKeys.INPUT])\n",
    "        batch[DataKeys.PREDS] = predictions\n",
    "        return batch\n",
    "\n",
    "    def modules_to_freeze(\n",
    "        self,\n",
    "    ) -> Union[nn.Module, Iterable[Union[nn.Module, Iterable]]]:\n",
    "        \"\"\"Return the module attributes of the model to be frozen.\"\"\"\n",
    "        return list(self.backbone.children())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17502246",
   "metadata": {},
   "source": [
    "## 4. Define the DataModule\n",
    "\n",
    "We will use the _.from_csv_ method while also writing the input variable: **videos** along with the list of **labels**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c366f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafay_farhan/.local/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py:32: LightningDeprecationWarning: `pytorch_lightning.utilities.apply_func.apply_to_collection` has been deprecated in v1.8.0 and will be removed in v2.0.0. Please use `lightning_utilities.core.apply_func.apply_to_collection` instead.\n",
      "  \"`pytorch_lightning.utilities.apply_func.apply_to_collection` has been deprecated in v1.8.0 and will be\"\n"
     ]
    }
   ],
   "source": [
    "# | output : False\n",
    "\n",
    "datamodule = VideoClassificationData.from_csv(\n",
    "    \"video\",\n",
    "    [\"archery\", \"bowling\", \"flying_kite\", \"high_jump\", \"marching\", \"indoor\", \"outdoor\"],\n",
    "    train_file=\"./data/kinetics/train/train.csv\",\n",
    "    train_videos_root=\"./data/kinetics/train/\",\n",
    "    val_file=\"./data/kinetics/val/val.csv\",\n",
    "    val_videos_root=\"./data/kinetics/val/\",\n",
    "    transform_kwargs=dict(image_size=(244, 244)),\n",
    "    clip_sampler=\"uniform\",\n",
    "    clip_duration=1,\n",
    "    decode_audio=False,\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    "    transform=TransformDataModule(),\n",
    "    persistent_workers=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ff5bf",
   "metadata": {},
   "source": [
    "Let's check if our datamodule is exactly what we need starting off with the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1954a592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archery',\n",
       " 'bowling',\n",
       " 'flying_kite',\n",
       " 'high_jump',\n",
       " 'marching',\n",
       " 'indoor',\n",
       " 'outdoor']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641dcf3",
   "metadata": {},
   "source": [
    "Looks good, it shows all 7 labels. The datamodule also has .multi_label attribute which is a boolean value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86fd431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.multi_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369686b",
   "metadata": {},
   "source": [
    "Great ! Exactly what we wanted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d99602",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "We have almost reached the summit! Before we start to train our model, we need to create it first. Let's define the evaluation metrics our model will follow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb334e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = (F1Score(num_labels=datamodule.num_classes, task=\"multilabel\", top_k=1))\n",
    "metrics = MultilabelAccuracy(num_labels, threshold=0.5, average=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75100f",
   "metadata": {},
   "source": [
    "Another crucial aspect is to know which activation function to use. For a multi-class classification problem, we use Softmax activation function. This is because we want to maximize the probability of a single class, and softmax ensures that the sum of the probabilities is one. However, we use Sigmoid activation function for the output layer in the multi-label classification setting. What sigmoid does is that it allows you to have a high probability for all your classes or some of them, or none of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a21ee97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(in_features=400, out_features=7, bias=True),\n",
    "    nn.Sigmoid(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765313f",
   "metadata": {},
   "source": [
    "Now, let's combine all of it together to create a model for training by using our custom **VC** class with a **x3d_m** backbone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4a61d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafay_farhan/.local/lib/python3.7/site-packages/pytorch_lightning/utilities/parsing.py:270: UserWarning: Attribute 'metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['metrics'])`.\n",
      "  f\"Attribute {k!r} is an instance of `nn.Module` and is already saved during checkpointing.\"\n",
      "/home/rafay_farhan/.local/lib/python3.7/site-packages/pytorch_lightning/utilities/parsing.py:270: UserWarning: Attribute 'head' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['head'])`.\n",
      "  f\"Attribute {k!r} is an instance of `nn.Module` and is already saved during checkpointing.\"\n",
      "Using 'x3d_m' provided by Facebook Research/PyTorchVideo (https://github.com/facebookresearch/pytorchvideo).\n"
     ]
    }
   ],
   "source": [
    "# | output: False\n",
    "\n",
    "model = VC(\n",
    "    backbone=\"x3d_m\",\n",
    "    labels=datamodule.labels,\n",
    "    metrics=metrics,\n",
    "    loss_fn=binary_cross_entropy_with_logits,\n",
    "    head=head,\n",
    "    multi_label=datamodule.multi_label,\n",
    "    pretrained=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3d275a",
   "metadata": {},
   "source": [
    "Some necessary checks to see if our model meets our requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "defacb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archery',\n",
       " 'bowling',\n",
       " 'flying_kite',\n",
       " 'high_jump',\n",
       " 'marching',\n",
       " 'indoor',\n",
       " 'outdoor']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "628820bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.multi_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f62c9aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=400, out_features=7, bias=True)\n",
       "  (2): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f9124",
   "metadata": {},
   "source": [
    "Great! Time for the training process to commence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c951149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/rafay_farhan/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:68: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  \"Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning`\"\n"
     ]
    }
   ],
   "source": [
    "# | output: False\n",
    "\n",
    "\n",
    "trainer = flash.Trainer(\n",
    "    max_epochs=2,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24097c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | train_metrics | ModuleDict | 0     \n",
      "1 | val_metrics   | ModuleDict | 0     \n",
      "2 | test_metrics  | ModuleDict | 0     \n",
      "3 | backbone      | Net        | 3.8 M \n",
      "4 | head          | Sequential | 2.8 K \n",
      "---------------------------------------------\n",
      "34.2 K    Trainable params\n",
      "3.8 M     Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.188    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafay_farhan/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "/home/rafay_farhan/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b9f1a5640b45e5bec9530febeb395d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "# | output: False\n",
    "\n",
    "trainer.finetune(model, datamodule=datamodule, strategy=\"freeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a500e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
